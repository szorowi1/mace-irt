{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9882b96c-b11d-4dec-b4c4-fee41a297dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame, concat, read_csv\n",
    "from arviz import hdi\n",
    "sns.set_theme(style='white', context='notebook', font_scale=1.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128e812-554a-40c0-9c94-0e0eca8c9cc4",
   "metadata": {},
   "source": [
    "## Section 1: Factor Loadings\n",
    "\n",
    "### Model 1: Unidimensional model of maltreatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537d25d-518c-4d97-91c4-ff212fe712c2",
   "metadata": {},
   "source": [
    "#### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c7c8cd8-e137-4205-b28c-c2cf419ada69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t1.357 [1.295, 1.414]\n",
      "alpha_min:\t0.506 [0.067, 0.901]\n",
      "alpha_max:\t2.944 [2.505, 3.392]\n",
      "lambda_mu:\t0.597 [0.579, 0.614]\n",
      "lambda_min:\t0.279 [0.039, 0.468]\n",
      "lambda_max:\t0.864 [0.829, 0.895]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t1.349 [1.272, 1.422]\n",
      "alpha_min:\t0.657 [0.443, 0.864]\n",
      "alpha_max:\t2.407 [1.982, 2.873]\n",
      "lambda_mu:\t0.600 [0.578, 0.621]\n",
      "lambda_min:\t0.359 [0.256, 0.457]\n",
      "lambda_max:\t0.814 [0.763, 0.863]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m1.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "        \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=param).values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286ec0c-ad20-4747-8caf-503a08faa05f",
   "metadata": {},
   "source": [
    "#### Proportion of strong loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d38b238-216b-4d0f-899a-01f46070f32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teicher2015:\t0.802 [0.718, 0.872]\n",
      "tuominen2022:\t0.800 [0.692, 0.872]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m1.tsv.gz'), sep='\\t', compression='gzip')\n",
    "            \n",
    "    ## Extract parameters.\n",
    "    arr = samples.filter(regex='lambda').values\n",
    "    \n",
    "    ## Compute proportion of strong loadings.\n",
    "    prop = (arr > 0.5).mean(axis=1)\n",
    "    mu = prop.mean(); lb, ub = hdi(prop, hdi_prob=0.95)\n",
    "    \n",
    "    ## Summarize information.\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(study, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7c500-1922-4faf-9cdc-adbb86a8f814",
   "metadata": {},
   "source": [
    "#### Comparison across datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0d22ca-ce46-483e-a9ff-24668ac50466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:\t0.565 [0.414, 0.711]\n",
      "lambda:\t0.565 [0.414, 0.711]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "for param in ['alpha','lambda']:\n",
    "\n",
    "    ## Load item parameters (teicher2015).\n",
    "    a1 = read_csv(os.path.join('stan_results', 'teicher2015', 'grmq_m1.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=param).values\n",
    "\n",
    "    ## Load item parameters (tuominen2022).\n",
    "    a2 = read_csv(os.path.join('stan_results', 'tuominen2022', 'grmq_m1.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=param).values\n",
    "\n",
    "    ## Compute rank correlation.\n",
    "    corr = np.array([spearmanr(i,j)[0] for i, j in zip(a1, a2)])\n",
    "    mu = corr.mean(); lb, ub = hdi(corr, hdi_prob=0.95)\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94befcf6-d037-4ecf-846c-eb0df7c03999",
   "metadata": {},
   "source": [
    "#### Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa452413-bc4c-45ec-b66d-2daa1999dbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.62  3.77  2.64  2.11  1.79  1.35]\n",
      "[13.27  4.43  2.48  2.04  1.81  1.56]\n"
     ]
    }
   ],
   "source": [
    "## Load polychoric correlation matrix.\n",
    "poly = read_csv(os.path.join('stan_results', 'polychoric.csv'))\n",
    "\n",
    "for study in ['teicher2015', 'tuominen2022']:\n",
    "    corr = poly.pivot_table(study, 'k1', 'k2').values\n",
    "    _, eig, _ = np.linalg.svd(corr)\n",
    "    print(eig.round(2)[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d224b-2610-49aa-88ca-7cd33bd7da25",
   "metadata": {},
   "source": [
    "### Model 2: Bifactor model (w/ 10 specific factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c264e4c-e57b-43f3-bf84-702ff95f7a80",
   "metadata": {},
   "source": [
    "#### Model parameters (general factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b456833-1667-4835-ad67-21a7c3912d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t1.503 [1.433, 1.579]\n",
      "alpha_min:\t0.470 [0.000, 0.830]\n",
      "alpha_max:\t4.248 [3.464, 4.996]\n",
      "lambda_mu:\t0.575 [0.556, 0.593]\n",
      "lambda_min:\t0.235 [0.000, 0.412]\n",
      "lambda_max:\t0.864 [0.829, 0.898]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t1.524 [1.434, 1.614]\n",
      "alpha_min:\t0.686 [0.430, 0.929]\n",
      "alpha_max:\t3.510 [2.364, 4.692]\n",
      "lambda_mu:\t0.580 [0.560, 0.601]\n",
      "lambda_min:\t0.299 [0.205, 0.393]\n",
      "lambda_max:\t0.827 [0.770, 0.877]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m2.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "        \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=f'{param}\\[[0-9]*,1\\]').values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadc379-4a68-4f2a-bda7-70c4e8a8707c",
   "metadata": {},
   "source": [
    "#### Model parameters (specific factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632b1c14-976e-4028-b89e-119515123971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t0.804 [0.738, 0.873]\n",
      "alpha_min:\t0.035 [0.000, 0.113]\n",
      "alpha_max:\t2.916 [2.325, 3.589]\n",
      "lambda_mu:\t0.313 [0.285, 0.341]\n",
      "lambda_min:\t0.014 [0.000, 0.046]\n",
      "lambda_max:\t0.797 [0.743, 0.851]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t0.903 [0.821, 0.977]\n",
      "alpha_min:\t0.090 [0.000, 0.240]\n",
      "alpha_max:\t3.135 [1.822, 4.456]\n",
      "lambda_mu:\t0.343 [0.315, 0.369]\n",
      "lambda_min:\t0.043 [0.000, 0.138]\n",
      "lambda_max:\t0.698 [0.606, 0.788]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m2.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "                \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=f'{param}\\[[0-9]*,[^1]|lambda\\[[0-9]*,1[0-2]').values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1c001-a2d9-494f-bbe7-7f2b94581584",
   "metadata": {},
   "source": [
    "#### Relative parameter bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a39f338-bfcf-48dd-8f9d-f92996ab5ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teicher2015:\t0.021 [-0.024, 0.066]\n",
      "tuominen2022:\t0.027 [-0.026, 0.080]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    \n",
    "    l1 = read_csv(os.path.join('stan_results', study, 'grmq_m1.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=f'lambda\\[[0-9]*,1\\]').values\n",
    "    l2 = read_csv(os.path.join('stan_results', study, 'grmq_m2.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=f'lambda\\[[0-9]*,1\\]').values\n",
    "    \n",
    "    ## Compute relative bias.\n",
    "    bias = np.median((l1 - l2) / l2, axis=1)\n",
    "    mu = np.median(bias); lb, ub = hdi(bias, hdi_prob=0.95)\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(study, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d56c6-2ce0-4a1b-a70f-69b6ec7434f1",
   "metadata": {},
   "source": [
    "### Model 3: Bifactor S-1 model (threat-deprivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad95329-f524-4df3-b467-04145ed01bd1",
   "metadata": {},
   "source": [
    "#### Model parameters (general factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e71914e-6f1d-4b7b-b7a7-41d1aef25028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t1.348 [1.285, 1.408]\n",
      "alpha_min:\t0.320 [0.000, 0.665]\n",
      "alpha_max:\t2.944 [2.485, 3.384]\n",
      "lambda_mu:\t0.577 [0.558, 0.594]\n",
      "lambda_min:\t0.150 [0.000, 0.310]\n",
      "lambda_max:\t0.864 [0.832, 0.899]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t1.408 [1.331, 1.494]\n",
      "alpha_min:\t0.735 [0.533, 0.952]\n",
      "alpha_max:\t4.055 [3.164, 4.905]\n",
      "lambda_mu:\t0.580 [0.558, 0.601]\n",
      "lambda_min:\t0.395 [0.299, 0.488]\n",
      "lambda_max:\t0.831 [0.780, 0.884]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m3.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "        \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=f'{param}\\[[0-9]*,1\\]').values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3e3a2-37d9-47b6-b328-b528dc213fbb",
   "metadata": {},
   "source": [
    "#### Model parameters (specific factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee33213-cdbd-43b5-81ee-3ec11ce8facc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t0.982 [0.865, 1.114]\n",
      "alpha_min:\t0.064 [0.000, 0.196]\n",
      "alpha_max:\t1.552 [1.207, 1.907]\n",
      "lambda_mu:\t0.410 [0.373, 0.454]\n",
      "lambda_min:\t0.030 [0.000, 0.091]\n",
      "lambda_max:\t0.617 [0.534, 0.699]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t1.497 [1.310, 1.684]\n",
      "alpha_min:\t0.197 [0.000, 0.407]\n",
      "alpha_max:\t4.080 [3.192, 4.976]\n",
      "lambda_mu:\t0.454 [0.409, 0.499]\n",
      "lambda_min:\t0.092 [0.000, 0.188]\n",
      "lambda_max:\t0.704 [0.625, 0.772]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m3.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "                \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=f'{param}\\[[0-9]*,[^1]|lambda\\[[0-9]*,1[0-2]').values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18408c1-d50c-4d94-a6f7-b1fa80754e6c",
   "metadata": {},
   "source": [
    "#### Relative parameter bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e9dbfd9-b512-45e5-99f6-592701b6bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teicher2015:\t0.010 [-0.026, 0.049]\n",
      "tuominen2022:\t0.011 [-0.039, 0.067]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    \n",
    "    l1 = read_csv(os.path.join('stan_results', study, 'grmq_m1.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=f'lambda\\[[0-9]*,1\\]').values\n",
    "    l2 = read_csv(os.path.join('stan_results', study, 'grmq_m3.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=f'lambda\\[[0-9]*,1\\]').values\n",
    "    \n",
    "    ## Compute relative bias.\n",
    "    bias = np.median((l1 - l2) / l2, axis=1)\n",
    "    mu = np.median(bias); lb, ub = hdi(bias, hdi_prob=0.95)\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(study, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e38858-f0a4-42c1-addc-c2a5e443b3fe",
   "metadata": {},
   "source": [
    "### Model 4: Bifactor S-1 model (parimonious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364def1c-720c-41e0-974e-4e91c984ddd1",
   "metadata": {},
   "source": [
    "#### Model parameters (general factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f490292-0e25-464b-a67c-cde3ae146bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t1.387 [1.320, 1.447]\n",
      "alpha_min:\t0.341 [0.000, 0.684]\n",
      "alpha_max:\t2.937 [2.494, 3.440]\n",
      "lambda_mu:\t0.563 [0.545, 0.580]\n",
      "lambda_min:\t0.158 [0.000, 0.311]\n",
      "lambda_max:\t0.864 [0.828, 0.898]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t1.440 [1.356, 1.520]\n",
      "alpha_min:\t0.657 [0.402, 0.887]\n",
      "alpha_max:\t4.175 [3.374, 4.988]\n",
      "lambda_mu:\t0.572 [0.550, 0.593]\n",
      "lambda_min:\t0.298 [0.199, 0.393]\n",
      "lambda_max:\t0.852 [0.803, 0.896]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m4.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "        \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=f'{param}\\[[0-9]*,1\\]').values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da6bbe-7e60-48f1-bd93-7f97030c25fa",
   "metadata": {},
   "source": [
    "#### Model parameters (specific factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc5b2541-6aa1-481a-9960-d399cc3f2749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study = teicher2015\n",
      "alpha_mu:\t1.402 [1.306, 1.495]\n",
      "alpha_min:\t0.655 [0.419, 0.918]\n",
      "alpha_max:\t2.746 [2.216, 3.274]\n",
      "lambda_mu:\t0.550 [0.522, 0.576]\n",
      "lambda_min:\t0.327 [0.214, 0.441]\n",
      "lambda_max:\t0.789 [0.737, 0.840]\n",
      "\n",
      "study = tuominen2022\n",
      "alpha_mu:\t1.526 [1.388, 1.658]\n",
      "alpha_min:\t0.363 [0.020, 0.646]\n",
      "alpha_max:\t3.744 [2.799, 4.722]\n",
      "lambda_mu:\t0.526 [0.492, 0.556]\n",
      "lambda_min:\t0.183 [0.019, 0.326]\n",
      "lambda_max:\t0.684 [0.588, 0.773]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    if i: print('')\n",
    "    print(f'study = {study}')\n",
    "\n",
    "    ## Load samples.\n",
    "    samples = read_csv(os.path.join('stan_results', study, 'grmq_m4.tsv.gz'), sep='\\t', compression='gzip')\n",
    "    \n",
    "    for param in ['alpha','lambda']:\n",
    "                \n",
    "        ## Extract parameters.\n",
    "        arr = samples.filter(regex=f'{param}\\[[0-9]*,[^1]|lambda\\[[0-9]*,1[0-2]').values\n",
    "        \n",
    "        ## Summarize across items.\n",
    "        mu = arr.mean(axis=1).mean(); lb, ub = hdi(arr.mean(axis=1), hdi_prob=0.95)\n",
    "        print('%s_mu:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize least discriminating.\n",
    "        ix = np.argmin(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_min:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))\n",
    "\n",
    "        ## Summarize most discriminating.\n",
    "        ix = np.argmax(arr.mean(axis=0))\n",
    "        mu = arr[:,ix].mean(); lb, ub = hdi(arr[:,ix], hdi_prob=0.95)\n",
    "        print('%s_max:\\t%0.3f [%0.3f, %0.3f]' %(param, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25454d-6f10-4a84-bf8b-928bd028c772",
   "metadata": {},
   "source": [
    "#### Relative parameter bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe1270be-10c0-40dc-8253-3ec71a173cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teicher2015:\t0.024 [-0.020, 0.074]\n",
      "tuominen2022:\t0.035 [-0.020, 0.100]\n"
     ]
    }
   ],
   "source": [
    "## Define I/O parameters.\n",
    "studies = ['teicher2015','tuominen2022']\n",
    "\n",
    "## Main loop.\n",
    "for i, study in enumerate(studies):\n",
    "    \n",
    "    l1 = read_csv(os.path.join('stan_results', study, 'grmq_m1.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=f'lambda\\[[0-9]*,1\\]').values\n",
    "    l2 = read_csv(os.path.join('stan_results', study, 'grmq_m4.tsv.gz'), sep='\\t', \n",
    "                  compression='gzip').filter(regex=f'lambda\\[[0-9]*,1\\]').values\n",
    "    \n",
    "    ## Compute relative bias.\n",
    "    bias = np.median((l1 - l2) / l2, axis=1)\n",
    "    mu = np.median(bias); lb, ub = hdi(bias, hdi_prob=0.95)\n",
    "    print('%s:\\t%0.3f [%0.3f, %0.3f]' %(study, mu, lb, ub))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c8042-9917-42d0-873b-b3098d429507",
   "metadata": {},
   "source": [
    "## Section 2: Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b464a99d-c191-40f3-a85d-df821baa1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load design data.\n",
    "design = read_csv(os.path.join('data', 'design.csv'), index_col=0)\n",
    "\n",
    "## Define locally dependent items.\n",
    "ld = [[6,7,8], [9,10,11], [13,14], [15,16], [19,20], [21,22,23], [24,25], [33,34,35], [36,37]]\n",
    "for ix in ld: design = design.drop(index=ix[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00a5e5-b7d2-4e22-adeb-b35a7ac03a67",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41df98d6-5060-443b-9160-44679546b8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>puc</th>\n",
       "      <th>ecv</th>\n",
       "      <th>omega</th>\n",
       "      <th>omega_s</th>\n",
       "      <th>H</th>\n",
       "      <th>ecv</th>\n",
       "      <th>omega</th>\n",
       "      <th>omega_s</th>\n",
       "      <th>H</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subscale</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>general</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VA</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.273</td>\n",
       "      <td></td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.052</td>\n",
       "      <td></td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVEA</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.525</td>\n",
       "      <td></td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SA</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.134</td>\n",
       "      <td></td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EN</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.304</td>\n",
       "      <td></td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PN</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.479</td>\n",
       "      <td></td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSV</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.053</td>\n",
       "      <td></td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WIPV</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.197</td>\n",
       "      <td></td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PeerVA</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.818</td>\n",
       "      <td></td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PeerPA</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.443</td>\n",
       "      <td></td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "study         1                                   2                      \n",
       "            puc    ecv  omega omega_s      H    ecv  omega omega_s      H\n",
       "subscale                                                                 \n",
       "general   0.912  0.716  0.963   0.924  0.964  0.697  0.965   0.924  0.961\n",
       "VA                      0.909   0.069  0.273         0.893   0.162  0.478\n",
       "PA                      0.590   0.037  0.052         0.595   0.148  0.194\n",
       "NVEA                    0.848   0.136  0.525         0.827   0.117  0.424\n",
       "SA                      0.710   0.059  0.134         0.749   0.290  0.452\n",
       "EN                      0.715   0.162  0.304         0.874   0.203  0.542\n",
       "PN                      0.800   0.217  0.479         0.812   0.114  0.284\n",
       "WSV                     0.695   0.027  0.053         0.651   0.037  0.067\n",
       "WIPV                    0.655   0.146  0.197         0.612   0.077  0.107\n",
       "PeerVA                  0.878   0.621  0.818         0.853   0.565  0.766\n",
       "PeerPA                  0.699   0.335  0.443         0.694   0.337  0.446"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define I/O parameters.\n",
    "studies = ['teicher2015', 'tuominen2022']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Restrict to columns of interest.\n",
    "D = design[design.columns[:11]].copy()\n",
    "\n",
    "stats = []\n",
    "for study in studies:\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load Stan summary.\n",
    "    summary = read_csv(os.path.join('stan_results', study, 'grmq_m2_summary.tsv'), sep='\\t', index_col=0)\n",
    "    \n",
    "    ## Extract factor loadings.\n",
    "    loadings = np.zeros_like(D).astype(float)\n",
    "    for i, j in np.column_stack([np.where(D)]).T:\n",
    "        loadings[i,j] = summary.loc[f'lambda[{i+1},{j+1}]','Mean']\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Coefficient omega hierachical.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "    ## Preallocate space.\n",
    "    omega   = np.zeros(len(D.columns))\n",
    "    omega_s = np.zeros(len(D.columns))\n",
    "        \n",
    "    ## Iterate over factors.\n",
    "    for i, col in enumerate(D.columns):\n",
    "        \n",
    "        ## Restrict to items in group.\n",
    "        L = loadings[D[col]==1]\n",
    "        \n",
    "        ## Compute squared sum of factor loadings.\n",
    "        A = np.square(np.sum(L, axis=0))\n",
    "        \n",
    "        ## Compute sum of error variances.\n",
    "        B = np.sum(1 - np.square(L).sum(axis=1))\n",
    "        \n",
    "        ## Compute total variance.\n",
    "        C = np.sum(A) + B\n",
    "        \n",
    "        ## Compute coefficient omega.\n",
    "        omega[i] = A.sum() / C\n",
    "        \n",
    "        ## Compute coefficient omega subscale.\n",
    "        omega_s[i] = A[i] / C\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Explained common variance.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "    ## Compute sum of squares.\n",
    "    ss = np.square(loadings).sum(axis=0)\n",
    "    \n",
    "    ## Compute explained common variance.\n",
    "    ecv = ss / ss.sum()\n",
    "    \n",
    "    ## Compute PUC.\n",
    "    A = len(D) * (len(D) - 1) / 2\n",
    "    B = sum([sum(D[col]) * (sum(D[col])-1) / 2 for col in D.columns[1:]])\n",
    "    puc = (A - B) / A\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### H-index\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Preallocate space.\n",
    "    H = np.zeros(len(D.columns))\n",
    "    \n",
    "    ## Iterate over factors.\n",
    "    for i, col in enumerate(D.columns):\n",
    "        \n",
    "        ## Compute squared loadings.\n",
    "        s = np.square(loadings[:,i])\n",
    "        \n",
    "        ## Compute H-index.\n",
    "        H[i] = 1. / (1 + 1 / np.sum(s / (1-s)))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Convert to DataFrame.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    stats.append(DataFrame(dict(\n",
    "        subscale = D.columns,\n",
    "        study = np.repeat(study, D.columns.size),\n",
    "        ecv = ecv,\n",
    "        puc = puc,\n",
    "        omega = omega,\n",
    "        omega_s = omega_s,\n",
    "        H = H\n",
    "    )))\n",
    "    \n",
    "## Concatenate DataFrames.\n",
    "stats = concat(stats).replace({'teicher2015':1, 'tuominen2022': 2})\n",
    "\n",
    "## Convert to pivot table.\n",
    "stats = stats.pivot_table(['omega','omega_s','ecv','puc','H'], 'subscale', 'study').round(3)\n",
    "stats = stats.applymap(lambda x: '%0.3f' %x)\n",
    "stats.loc[stats.index!='general',['ecv','puc']] = ''\n",
    "\n",
    "## Re-organize rows.\n",
    "index = ['general', 'VA', 'PA', 'NVEA', 'SA', 'EN', 'PN', 'WSV', 'WIPV', 'PeerVA', 'PeerPA']\n",
    "stats = stats.loc[index]\n",
    "\n",
    "## Re-organize columns.\n",
    "cols = [(1,'puc'),(1,'ecv'),(1,'omega'),(1,'omega_s'),(1,'H'),\n",
    "        (2,'ecv'),(2,'omega'),(2,'omega_s'),(2,'H')]\n",
    "stats = stats.swaplevel(axis='columns')[cols]\n",
    "\n",
    "## Display table.\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17246213-7763-42b3-a351-e473f0e2f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "study & \\multicolumn{5}{l}{1} & \\multicolumn{4}{l}{2} \\\\\n",
      "{} &    puc &    ecv &  omega & omega\\_s &      H &    ecv &  omega & omega\\_s &      H \\\\\n",
      "subscale &        &        &        &         &        &        &        &         &        \\\\\n",
      "\\midrule\n",
      "general  &  0.912 &  0.716 &  0.963 &   0.924 &  0.964 &  0.697 &  0.965 &   0.924 &  0.961 \\\\\n",
      "VA       &        &        &  0.909 &   0.069 &  0.273 &        &  0.893 &   0.162 &  0.478 \\\\\n",
      "PA       &        &        &  0.590 &   0.037 &  0.052 &        &  0.595 &   0.148 &  0.194 \\\\\n",
      "NVEA     &        &        &  0.848 &   0.136 &  0.525 &        &  0.827 &   0.117 &  0.424 \\\\\n",
      "SA       &        &        &  0.710 &   0.059 &  0.134 &        &  0.749 &   0.290 &  0.452 \\\\\n",
      "EN       &        &        &  0.715 &   0.162 &  0.304 &        &  0.874 &   0.203 &  0.542 \\\\\n",
      "PN       &        &        &  0.800 &   0.217 &  0.479 &        &  0.812 &   0.114 &  0.284 \\\\\n",
      "WSV      &        &        &  0.695 &   0.027 &  0.053 &        &  0.651 &   0.037 &  0.067 \\\\\n",
      "WIPV     &        &        &  0.655 &   0.146 &  0.197 &        &  0.612 &   0.077 &  0.107 \\\\\n",
      "PeerVA   &        &        &  0.878 &   0.621 &  0.818 &        &  0.853 &   0.565 &  0.766 \\\\\n",
      "PeerPA   &        &        &  0.699 &   0.335 &  0.443 &        &  0.694 &   0.337 &  0.446 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-eee063aaf1d3>:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(stats.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(stats.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090511d-3aed-4a76-9f5c-32ead715b3fa",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ea5195b-2b40-4d3d-a18d-dd96fedc46fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>puc</th>\n",
       "      <th>ecv</th>\n",
       "      <th>omega</th>\n",
       "      <th>omega_s</th>\n",
       "      <th>H</th>\n",
       "      <th>ecv</th>\n",
       "      <th>omega</th>\n",
       "      <th>omega_s</th>\n",
       "      <th>H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>general</th>\n",
       "      <td>0.939</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neglect</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.766</td>\n",
       "      <td></td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "study        1                                   2                      \n",
       "           puc    ecv  omega omega_s      H    ecv  omega omega_s      H\n",
       "general  0.939  0.864  0.958   0.928  0.964  0.841  0.959   0.922  0.959\n",
       "neglect                0.877   0.384  0.766         0.921   0.375  0.814"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define I/O parameters.\n",
    "studies = ['teicher2015', 'tuominen2022']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Restrict to columns of interest.\n",
    "D = design[['general','neglect']].copy()\n",
    "\n",
    "stats = []\n",
    "for study in studies:\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load Stan summary.\n",
    "    summary = read_csv(os.path.join('stan_results', study, 'grmq_m3_summary.tsv'), sep='\\t', index_col=0)\n",
    "    \n",
    "    ## Extract factor loadings.\n",
    "    loadings = np.zeros_like(D).astype(float)\n",
    "    for i, j in np.column_stack([np.where(D)]).T:\n",
    "        loadings[i,j] = summary.loc[f'lambda[{i+1},{j+1}]','Mean']\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Coefficient omega hierachical.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "    ## Preallocate space.\n",
    "    omega   = np.zeros(len(D.columns))\n",
    "    omega_s = np.zeros(len(D.columns))\n",
    "        \n",
    "    ## Iterate over factors.\n",
    "    for i, col in enumerate(D.columns):\n",
    "        \n",
    "        ## Restrict to items in group.\n",
    "        L = loadings[D[col]==1]\n",
    "        \n",
    "        ## Compute squared sum of factor loadings.\n",
    "        A = np.square(np.sum(L, axis=0))\n",
    "        \n",
    "        ## Compute sum of error variances.\n",
    "        B = np.sum(1 - np.square(L).sum(axis=1))\n",
    "        \n",
    "        ## Compute total variance.\n",
    "        C = np.sum(A) + B\n",
    "        \n",
    "        ## Compute coefficient omega.\n",
    "        omega[i] = A.sum() / C\n",
    "        \n",
    "        ## Compute coefficient omega subscale.\n",
    "        omega_s[i] = A[i] / C\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Explained common variance.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "    ## Compute sum of squares.\n",
    "    ss = np.square(loadings).sum(axis=0)\n",
    "    \n",
    "    ## Compute explained common variance.\n",
    "    ecv = ss / ss.sum()\n",
    "    \n",
    "    ## Compute PUC.\n",
    "    A = len(D) * (len(D) - 1) / 2\n",
    "    B = sum([sum(D[col]) * (sum(D[col])-1) / 2 for col in D.columns[1:]])\n",
    "    puc = (A - B) / A\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### H-index\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Preallocate space.\n",
    "    H = np.zeros(len(D.columns))\n",
    "    \n",
    "    ## Iterate over factors.\n",
    "    for i, col in enumerate(D.columns):\n",
    "        \n",
    "        ## Compute squared loadings.\n",
    "        s = np.square(loadings[:,i])\n",
    "        \n",
    "        ## Compute H-index.\n",
    "        H[i] = 1. / (1 + 1 / np.sum(s / (1-s)))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Convert to DataFrame.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    stats.append(DataFrame(dict(\n",
    "        subscale = D.columns,\n",
    "        study = np.repeat(study, D.columns.size),\n",
    "        ecv = ecv,\n",
    "        puc = puc,\n",
    "        omega = omega,\n",
    "        omega_s = omega_s,\n",
    "        H = H\n",
    "    )))\n",
    "    \n",
    "## Concatenate DataFrames.\n",
    "stats = concat(stats).replace({'teicher2015':1, 'tuominen2022': 2})\n",
    "\n",
    "## Convert to pivot table.\n",
    "stats = stats.pivot_table(['omega','omega_s','ecv','puc','H'], 'subscale', 'study').round(3)\n",
    "stats = stats.applymap(lambda x: '%0.3f' %x)\n",
    "stats.loc[stats.index!='general',['ecv','puc']] = ''\n",
    "\n",
    "## Re-organize rows.\n",
    "index = D.columns\n",
    "stats = stats.loc[index]\n",
    "\n",
    "## Re-organize columns.\n",
    "cols = [(1,'puc'),(1,'ecv'),(1,'omega'),(1,'omega_s'),(1,'H'),\n",
    "        (2,'ecv'),(2,'omega'),(2,'omega_s'),(2,'H')]\n",
    "stats = stats.swaplevel(axis='columns')[cols]\n",
    "\n",
    "## Display table.\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35f79c5-999b-4ec9-bca7-e5d304285f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-eee063aaf1d3>:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(stats.to_latex())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "study & \\multicolumn{5}{l}{1} & \\multicolumn{4}{l}{2} \\\\\n",
      "{} &    puc &    ecv &  omega & omega\\_s &      H &    ecv &  omega & omega\\_s &      H \\\\\n",
      "\\midrule\n",
      "general &  0.939 &  0.864 &  0.958 &   0.928 &  0.964 &  0.841 &  0.959 &   0.922 &  0.959 \\\\\n",
      "neglect &        &        &  0.877 &   0.384 &  0.766 &        &  0.921 &   0.375 &  0.814 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stats.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17311c4-a39b-402f-9d53-0f5a7624f2cf",
   "metadata": {},
   "source": [
    "### 2.3 Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c9a017b-0fc4-47f7-a576-d66793552e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>puc</th>\n",
       "      <th>ecv</th>\n",
       "      <th>omega</th>\n",
       "      <th>omega_s</th>\n",
       "      <th>H</th>\n",
       "      <th>ecv</th>\n",
       "      <th>omega</th>\n",
       "      <th>omega_s</th>\n",
       "      <th>H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>general</th>\n",
       "      <td>0.931</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peer</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.842</td>\n",
       "      <td></td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reverse</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.739</td>\n",
       "      <td></td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "study        1                                   2                      \n",
       "           puc    ecv  omega omega_s      H    ecv  omega omega_s      H\n",
       "general  0.931  0.738  0.961   0.896  0.965  0.751  0.961   0.904  0.960\n",
       "peer                   0.889   0.569  0.842         0.868   0.493  0.781\n",
       "reverse                0.839   0.584  0.739         0.915   0.498  0.773"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Define parameters.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Define I/O parameters.\n",
    "studies = ['teicher2015', 'tuominen2022']\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "### Main loop.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "## Restrict to columns of interest.\n",
    "D = design[['general','peer','reverse']].copy()\n",
    "\n",
    "stats = []\n",
    "for study in studies:\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Load and prepare data.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "    ## Load Stan summary.\n",
    "    summary = read_csv(os.path.join('stan_results', study, 'grmq_m4_summary.tsv'), sep='\\t', index_col=0)\n",
    "    \n",
    "    ## Extract factor loadings.\n",
    "    loadings = np.zeros_like(D).astype(float)\n",
    "    for i, j in np.column_stack([np.where(D)]).T:\n",
    "        loadings[i,j] = summary.loc[f'lambda[{i+1},{j+1}]','Mean']\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Coefficient omega hierachical.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "    ## Preallocate space.\n",
    "    omega   = np.zeros(len(D.columns))\n",
    "    omega_s = np.zeros(len(D.columns))\n",
    "        \n",
    "    ## Iterate over factors.\n",
    "    for i, col in enumerate(D.columns):\n",
    "        \n",
    "        ## Restrict to items in group.\n",
    "        L = loadings[D[col]==1]\n",
    "        \n",
    "        ## Compute squared sum of factor loadings.\n",
    "        A = np.square(np.sum(L, axis=0))\n",
    "        \n",
    "        ## Compute sum of error variances.\n",
    "        B = np.sum(1 - np.square(L).sum(axis=1))\n",
    "        \n",
    "        ## Compute total variance.\n",
    "        C = np.sum(A) + B\n",
    "        \n",
    "        ## Compute coefficient omega.\n",
    "        omega[i] = A.sum() / C\n",
    "        \n",
    "        ## Compute coefficient omega subscale.\n",
    "        omega_s[i] = A[i] / C\n",
    "        \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Explained common variance.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "        \n",
    "    ## Compute sum of squares.\n",
    "    ss = np.square(loadings).sum(axis=0)\n",
    "    \n",
    "    ## Compute explained common variance.\n",
    "    ecv = ss / ss.sum()\n",
    "    \n",
    "    ## Compute PUC.\n",
    "    A = len(D) * (len(D) - 1) / 2\n",
    "    B = sum([sum(D[col]) * (sum(D[col])-1) / 2 for col in D.columns[1:]])\n",
    "    puc = (A - B) / A\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### H-index\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    ## Preallocate space.\n",
    "    H = np.zeros(len(D.columns))\n",
    "    \n",
    "    ## Iterate over factors.\n",
    "    for i, col in enumerate(D.columns):\n",
    "        \n",
    "        ## Compute squared loadings.\n",
    "        s = np.square(loadings[:,i])\n",
    "        \n",
    "        ## Compute H-index.\n",
    "        H[i] = 1. / (1 + 1 / np.sum(s / (1-s)))\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ### Convert to DataFrame.\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    \n",
    "    stats.append(DataFrame(dict(\n",
    "        subscale = D.columns,\n",
    "        study = np.repeat(study, D.columns.size),\n",
    "        ecv = ecv,\n",
    "        puc = puc,\n",
    "        omega = omega,\n",
    "        omega_s = omega_s,\n",
    "        H = H\n",
    "    )))\n",
    "    \n",
    "## Concatenate DataFrames.\n",
    "stats = concat(stats).replace({'teicher2015':1, 'tuominen2022': 2})\n",
    "\n",
    "## Convert to pivot table.\n",
    "stats = stats.pivot_table(['omega','omega_s','ecv','puc','H'], 'subscale', 'study').round(3)\n",
    "stats = stats.applymap(lambda x: '%0.3f' %x)\n",
    "stats.loc[stats.index!='general',['ecv','puc']] = ''\n",
    "\n",
    "## Re-organize rows.\n",
    "index = D.columns\n",
    "stats = stats.loc[index]\n",
    "\n",
    "## Re-organize columns.\n",
    "cols = [(1,'puc'),(1,'ecv'),(1,'omega'),(1,'omega_s'),(1,'H'),\n",
    "        (2,'ecv'),(2,'omega'),(2,'omega_s'),(2,'H')]\n",
    "stats = stats.swaplevel(axis='columns')[cols]\n",
    "\n",
    "## Display table.\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc35d16-2cfd-4aef-a82a-cba62ef570e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "study & \\multicolumn{5}{l}{1} & \\multicolumn{4}{l}{2} \\\\\n",
      "{} &    puc &    ecv &  omega & omega\\_s &      H &    ecv &  omega & omega\\_s &      H \\\\\n",
      "\\midrule\n",
      "general &  0.931 &  0.738 &  0.961 &   0.896 &  0.965 &  0.751 &  0.961 &   0.904 &  0.960 \\\\\n",
      "peer    &        &        &  0.889 &   0.569 &  0.842 &        &  0.868 &   0.493 &  0.781 \\\\\n",
      "reverse &        &        &  0.839 &   0.584 &  0.739 &        &  0.915 &   0.498 &  0.773 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-eee063aaf1d3>:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(stats.to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(stats.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
